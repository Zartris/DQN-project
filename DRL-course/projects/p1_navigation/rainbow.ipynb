{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation and notes on rainbow DQN\n",
    "This will be some notes on how to build up the Rainbow DQN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. imports\n",
    "You need to have Pytorch + vision, UnityAgents, matplotlib, numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualize and data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "# Game env\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# Personal lib for logging:\n",
    "import utils.log as log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. initialise game\n",
    "The first parameter **use_test_seed** is need to be set to be able to run train or test. This is done this way to make sure i don't fat finger stuff and delete work :) Set it to what you need.\n",
    "\n",
    "After this is set you should be able to go to menu bar under **Cell** and click **run all** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# take test seed:\n",
    "use_test_seed = True\n",
    "test_seed = np.random.randint(low=1, high=1000)\n",
    "\n",
    "#Seed values from now on.\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Game values\n",
    "game = \"Banana.exe\"\n",
    "if use_test_seed:\n",
    "    env = UnityEnvironment(file_name=game, seed=(test_seed), no_graphics=False)\n",
    "else:\n",
    "    env = UnityEnvironment(file_name=game, seed=seed, no_graphics=True)\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# game info:\n",
    "action_size = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "\n",
    "general_info = log.create_general_info(\"*general info:*\", game, seed, state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters\n",
    "Here we go through all the hyperparameters\n",
    "### 2.1 model parameters\n",
    "Using a noisy DQN we need to have a initialised noise value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_init = 0.2                      # The amount of noise applied\n",
    "model_info = log.create_model_info(\"*model info:*\", std_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prioritised Experience Replay buffer parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = (2 ** 20)             # The space we use to store Experiences \n",
    "BATCH_SIZE = 512                    # Amount of replays we train on each update.\n",
    "RB_method = \"nstep_per\"             # Choice of replay buffer: nstep_per, (per, replay_buffer)=not_implemented\n",
    "PER_e = 0.01                        # Epsilon\n",
    "PER_a = 0.6                         # Alpha\n",
    "PER_b = 0.4                         # Beta init\n",
    "PER_bi = 0.00001                    # Beta increase is the increase in taking the most prioritiesed replays\n",
    "PER_aeu = 1                         # Absolute error upper is the max priority a replay can have\n",
    "PER_learn_start = 0                 # Used to populated the sumtree with replays\n",
    "n_step = 20                         # Used in the n-step implementation for choosing how many sequent replays we use.\n",
    "per_info = log.create_per_info(\"*per_info:*\", BUFFER_SIZE, BATCH_SIZE, \\\n",
    "                               RB_method, PER_e, PER_a, PER_b, PER_bi, PER_aeu, PER_learn_start, n_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Agent parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN agent\n",
    "GAMMA = 0.95                       # Future discount value\n",
    "TAU = 1e-3                         # Amount we update the target model each update session (use_soft_update=True)\n",
    "LR = 0.00005                       # The learning rate of the model\n",
    "opt_eps = 1.5e-4                   # Adam epsilon (more info)\n",
    "UPDATE_MODEL_EVERY = 10            # The amount of steps between model updates \n",
    "UPDATE_TARGET_EVERY = 8000         # The amount of steps between target updates (use_soft_update=Flase)\n",
    "use_soft_update = True             # Wether we are updating the model using soft updates or copying model weights over.\n",
    "priority_method = \"reward\"         # Initialised priorities (reward, none=max_val, error=compute_error)\n",
    "\n",
    "\n",
    "# Distributed \n",
    "atom_size = 51                     # Number of atoms\n",
    "v_max = 1                          # Max value for support\n",
    "v_min = -v_max                     # Min value for support (does not have to be -v_max but i found this to be good.)\n",
    "\n",
    "agent_info = log.create_agent_info(\"*agent info:*\", GAMMA, TAU, LR, opt_eps,\n",
    "                                   UPDATE_MODEL_EVERY, UPDATE_TARGET_EVERY, use_soft_update, priority_method,\n",
    "                                   atom_size, v_max, v_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1000                   # Number of training episodes\n",
    "evaluation_interval = 100         # Indicating how often we evaluate the current agent.\n",
    "max_t = 1000                      # The max number of steps before going into new episode (not used)\n",
    "train_info = log.create_train_info(\"*train_info:*\", episodes, evaluation_interval, max_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model and layers: Noisy Dueling DQN\n",
    "The paper of noisynet is [Noisy networks for exploration](https://arxiv.org/pdf/1706.10295.pdf) and the paper of [Dueling\n",
    "Noisy DQN](https://arxiv.org/pdf/1509.06461.pdf). \n",
    "\n",
    "In this section we create a factorized noisy layer to apply noise to the last layers hereby introducing exploration.\n",
    "Dueling DQN is that we split the last layer into two and compute an advangtage vector (How good are each action in this state) and a value (how good is the state).\n",
    "### 3.1 Noisy layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedNoisyLinear(nn.Module):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features,  # Number of input features\n",
    "                 out_features,  # Number of output features\n",
    "                 std_init,  # Amount of noise in layer\n",
    "                 seed=None,  # The env seed (if needed)\n",
    "                 name=\"noisyLinear\"  # Name for debugging\n",
    "                 ):\n",
    "        super(FactorizedNoisyLinear, self).__init__()\n",
    "        if seed is not None:\n",
    "            self.seed = seed\n",
    "            torch.manual_seed(self.seed)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.register_buffer(\"weight_epsilon\", torch.Tensor(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
    "\n",
    "        self.std_init = std_init\n",
    "        self.name = name\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Applying noise to the weights and bias to simulate exploring\n",
    "        :param x: The input state\n",
    "        :return: Linear translation with noisy weights and bias.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "\n",
    "        y = F.linear(x, weight, bias)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Quote from paper:\n",
    "        # For factorised noisy networks, each element µi,j was initialised by a sample from an independent\n",
    "        # uniform distributions U[− (1/ sqrt(p)), + (1/sqrt(p))]\n",
    "        # and each element σi,j was initialised to a constant sqrt(σ_0/p).\n",
    "        # The hyperparameter σ_0 is set to 0.5 (Not in our case).\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        std = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-std, std)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "\n",
    "        self.bias_mu.data.uniform_(-std, std)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"\n",
    "        Factorised Gaussian noise\n",
    "        From the paper:\n",
    "        # Each ε^{w}_{i,j} and ε^{b}_{j} can then be written as:\n",
    "        ε^{w}_{i,j} = f(ε_i) dot f(ε_j)\n",
    "        ε^{b}_{j} = f(ε_j)\n",
    "        where f is a real value function.\n",
    "        \"\"\"\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        # ε^{w}_{i,j} = f(ε_i) dot f(ε_j)\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in)) # Ger is outer product\n",
    "        # ε^{b}_{j} = f(ε_j)\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        \"\"\"\n",
    "        This is the chosen real value function f:\n",
    "        sig(x) * sqrt(|x|) as in the paper.\n",
    "        \"\"\"\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The Distributed Noisy DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedNoisyDDQN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 state_size: int,\n",
    "                 action_size: int,\n",
    "                 std_init: float,\n",
    "                 support: torch.Tensor,\n",
    "                 atom_size: int = 51,\n",
    "                 seed=None):\n",
    "        super(DistributedNoisyDDQN, self).__init__()\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        self.support = support\n",
    "        self.atom_size = atom_size\n",
    "\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.value_stream = nn.Sequential(\n",
    "            FactorizedNoisyLinear(512, 512, std_init=std_init, seed=seed, name=\"value_stream1\"),\n",
    "            nn.ReLU(),\n",
    "            FactorizedNoisyLinear(512, 1 * self.atom_size, std_init=std_init, seed=seed, name=\"value_stream2\")\n",
    "        )\n",
    "\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            FactorizedNoisyLinear(512, 512, std_init=std_init, seed=seed, name=\"advantage_stream1\"),\n",
    "            nn.ReLU(),\n",
    "            FactorizedNoisyLinear(512, self.action_size * self.atom_size, std_init=std_init, seed=seed,\n",
    "                                  name=\"advantage_stream2\")\n",
    "        )\n",
    "\n",
    "        self.feature_layer.apply(self.init_weights)\n",
    "        self.value_stream.apply(self.init_weights)\n",
    "        self.advantage_stream.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        # Compute dist:\n",
    "        distribution = self.get_distribution(state)\n",
    "        # get q_values:\n",
    "        q_values = torch.sum(distribution * self.support, dim=2)\n",
    "        return q_values\n",
    "\n",
    "    def get_distribution(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        # Run through model:\n",
    "        x = self.feature_layer(state)\n",
    "        value = self.value_stream(x).view(-1, 1, self.atom_size)\n",
    "        advantage = self.advantage_stream(x).view(-1, self.action_size, self.atom_size)\n",
    "        # Compute Q values for atoms\n",
    "        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "\n",
    "        dist = F.softmax(q_atoms, dim=-1)  # Using softmax when working with a distribution.\n",
    "        dist = dist.clamp(min=1e-3)  # To avoid nans\n",
    "        return dist\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, FactorizedNoisyLinear):\n",
    "                module.reset_noise()\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 N-step Prioritiesed Experience Replay buffer\n",
    "In **the Prioritiesed Experience Replay** (PER) buffer, we do not chose the experience replays at random but weights the less seen or valueable experiences with a higher priority to better cover the information given from this experience.\n",
    "\n",
    "**The N-step** implementation aims to improve the temporal awareness of the agent, by combining the information of current state and action with the next N-steps into the future. This way we try to learn if a state is contributing to future rewards.\n",
    "\n",
    "We store the priorities of the experiences in a sumtree for faster search and updates.\n",
    "### 4.1 Sumtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    def __init__(self, capacity, seed=None):\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        self.capacity = int(capacity)\n",
    "        assert self.is_power_of_2(self.capacity), \"Capacity must be power of 2.\" + str(capacity)\n",
    "        # pointer to current index in data map.\n",
    "        self.data_pointer = 0\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.data_length = 0\n",
    "        # Priority tree.\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_length\n",
    "\n",
    "    def add(self, data, priority):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        # Update the leaf\n",
    "        self.update(tree_index, priority)\n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "        if self.data_length < self.capacity:\n",
    "            self.data_length += 1\n",
    "\n",
    "    def update(self, tree_index, priority):\n",
    "        # change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        parent_index = 0  # root\n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "\n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else:  # downward search, always search for a higher priority node\n",
    "                if value <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    value -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, data_index, self.tree[leaf_index], self.data[data_index]\n",
    "\n",
    "    # Returns data given a data index\n",
    "    def get_data(self, data_index):\n",
    "        return self.data[data_index % self.capacity]\n",
    "\n",
    "    @staticmethod\n",
    "    def is_power_of_2(n):\n",
    "        return ((n & (n - 1)) == 0) and n != 0\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]  # the root\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 N-step PER buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerNStep:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __init__(self, capacity, batch_size, state_size, seed=None, epsilon=.001, alpha=.6, beta=.4, beta_increase=1e-3,\n",
    "                 absolute_error_upper=3, n_step=3, gamma=.99):\n",
    "        \"\"\"\n",
    "        :param capacity: Max amount of experience saved in the structure\n",
    "        :param epsilon: small value to insure all probabilities is not 0\n",
    "        :param alpha: introduces some randomness and to insure we don't train the same experience and overfit\n",
    "                      alpha=1 means greedy selecting the experience with highest priority\n",
    "                      alpha=0 means pure uniform randomness\n",
    "        :param beta: controls how much IS w affect learning\n",
    "                     beta>=0, starts close to 0 and get closer and closer to 1\n",
    "                     because these weights are more important in the end of learning when our q-values\n",
    "                     begins to convert\n",
    "        :param beta_increase: is the increase in beta for each sampling. 0.001 = 1e-3\n",
    "        :param absolute_error_upper: Setting a cap on how big an error (priority) can be.\n",
    "        :param n_step: store the most recent n-step transitions or experiences instead of the default 1.\n",
    "        :param gamma: This is the discount value\n",
    "        \"\"\"\n",
    "        ## Just like PER\n",
    "        # seeding\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "        # init\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.memory_tree = SumTree(self.capacity, seed)\n",
    "        self.experience = namedtuple(\"Experience\",\n",
    "                                     field_names=[\"timestep\", \"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_increase = beta_increase\n",
    "        self.absolute_error_upper = absolute_error_upper\n",
    "        self.seed = seed\n",
    "\n",
    "        ## N-Step\n",
    "        self.t = 0  # Internal time step counter\n",
    "        self.n_step = n_step\n",
    "        self.n_step_buff = deque(maxlen=n_step)\n",
    "        self.gamma = gamma\n",
    "        self.blank_experience = self.experience(timestep=0,\n",
    "                                                state=torch.zeros(state_size, dtype=torch.float64),\n",
    "                                                action=None,\n",
    "                                                reward=0,\n",
    "                                                next_state=torch.zeros(state_size, dtype=torch.float64),\n",
    "                                                done=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory_tree)\n",
    "\n",
    "    def is_full(self):\n",
    "        return len(self.memory_tree) >= self.memory_tree.capacity\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "                - First, to sample a minibatch of size k the range [0, priority_total] is divided into k ranges.\n",
    "                - Then a value is uniformly sampled from each range.\n",
    "                - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "                - Then, we calculate IS weights for each minibatch element.\n",
    "\n",
    "                The difference here from the last structure is that we need to move to device\n",
    "                in the method calling this function.\n",
    "\n",
    "                so an example:\n",
    "                idxs, experiences, is_weights = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "                states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(device)\n",
    "                actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).float().to(device)\n",
    "                rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(device)\n",
    "                next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(device)\n",
    "                dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "                is_weights =  torch.from_numpy(is_weights).float().to(device)\n",
    "                \"\"\"\n",
    "\n",
    "        minibatch = []\n",
    "\n",
    "        idxs = np.empty((self.batch_size,), dtype=np.int32)\n",
    "        is_weights = np.empty((self.batch_size,), dtype=np.float32)\n",
    "\n",
    "        # Calculate the priority segment\n",
    "        # Divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.memory_tree.total_priority / self.batch_size  # priority segment\n",
    "\n",
    "        # Increase the beta each time we sample a new minibatch\n",
    "        self.beta = np.amin([1., self.beta + self.beta_increase])  # max = 1\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            \"\"\"\n",
    "            A value is uniformly sampled from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "\n",
    "            # This while is to counter that we find a leaf that is not populated yet.\n",
    "            # It happens when the buffer size is very large.\n",
    "            leaf_index, data_index, priority, data = 0, 0, 0, 0\n",
    "            while data == 0:\n",
    "                value = np.random.uniform(a, b)\n",
    "\n",
    "                \"\"\"\n",
    "                Experience that corresponds to each value is retrieved\n",
    "                \"\"\"\n",
    "                leaf_index, data_index, priority, data = self.memory_tree.get_leaf(value)\n",
    "\n",
    "            # P(i) = p_i**a / sum_k p_k**a\n",
    "            sampling_probabilities = priority / self.memory_tree.total_priority\n",
    "\n",
    "            # (1/N * 1/P(i))**b and to normalize it we divide with max_weight\n",
    "            # So is_weights[i] = (1/N * 1/P(i))**b\n",
    "            is_weights[i] = np.power(self.batch_size * sampling_probabilities, -self.beta)\n",
    "\n",
    "            idxs[i] = leaf_index\n",
    "            minibatch.append(data)\n",
    "        is_weights /= is_weights.max()\n",
    "        return idxs, minibatch, is_weights\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done, error=None):\n",
    "        # TODO: Test if the computed priority is lower than the updated priorities\n",
    "        # This way we might never really train on new data since b_i is increasing.\n",
    "        exp = self.experience(self.t, torch.from_numpy(state), action, reward, torch.from_numpy(next_state), done)\n",
    "        self.n_step_buff.append(exp)\n",
    "        self.t = (0 if done else self.t + 1)\n",
    "        if len(self.n_step_buff) < self.n_step:\n",
    "            return None\n",
    "        exp, priority = self._get_n_step_info(self.n_step_buff, self.gamma)\n",
    "        priority = min((abs(priority) + self.epsilon) ** self.alpha, self.absolute_error_upper)\n",
    "        self.memory_tree.add(exp, priority)\n",
    "\n",
    "    def update_memory_tree(self, idxs, errors):\n",
    "        errors = errors + self.epsilon\n",
    "        clipped_errors = np.minimum(errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.alpha)\n",
    "\n",
    "        for idx, p in zip(idxs, ps):\n",
    "            self.memory_tree.update(idx, p)\n",
    "\n",
    "    def _get_n_step_info(self, n_step_buff, gamma):\n",
    "        timestep, org_state, org_action, _, _, _ = n_step_buff[0]\n",
    "        relevant_transitions = []\n",
    "        for transition in list(n_step_buff):\n",
    "            if timestep == transition.timestep:\n",
    "                relevant_transitions.append(transition)\n",
    "                timestep += 1\n",
    "            else:\n",
    "                break\n",
    "        # Take last element in deque and add the reward\n",
    "        rew, n_state, done = relevant_transitions[-1][-3:]\n",
    "        for transition in reversed(relevant_transitions[:-1]):\n",
    "            reward, n_s, d = transition.reward, transition.next_state, transition.done\n",
    "            rew = reward + gamma * rew * (1 - done)\n",
    "            n_state, done = (n_s, d) if d else (n_state, done)\n",
    "        return self.experience(timestep, org_state, org_action, rew, n_state, done), rew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distributional Double DQN Agent\n",
    "The paper of for [Distributional DQN](https://arxiv.org/pdf/1707.06887.pdf) introduces a distribution of future rewards associated with each action in a given state Z(s,a) instead of a single expected value Q(s,a). \n",
    "\n",
    "In the paper for [Double DQN](https://arxiv.org/abs/1509.06461) they try to tend to the problem of overestimation of action values, by introducting two models. A **model** that is trained on every update call we do, we use this model to select what is the best action to take for the next state (the action with the highest Q value). Then we have a **model_target** which is only updated partly on each update call (soft_update) or copied over from the **model** every hard_update steps.\n",
    "\n",
    "It is called RainbowAgent because all of these techniques combined is a RainbowAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowAgent:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_size: int,\n",
    "                 action_size: int,\n",
    "                 models: list,\n",
    "                 replay_buffer: PerNStep,\n",
    "                 seed: int = None,\n",
    "                 BATCH_SIZE: int = 64,\n",
    "                 GAMMA: float = 0.99,\n",
    "                 TAU: float = 1e-3,\n",
    "                 LR: float = 5e-4,\n",
    "                 UPDATE_MODEL_EVERY: int = 4,\n",
    "                 UPDATE_TARGET_EVERY: int = 1000,\n",
    "                 use_soft_update: bool = False,\n",
    "                 priority_method: str = \"reward\",\n",
    "                 # PER\n",
    "                 PER_learn_start: int = 0,\n",
    "                 PER_eps: float = 1e-6,\n",
    "                 # N-step\n",
    "                 n_step: int = 3,\n",
    "                 # Distributed\n",
    "                 atom_size: int = 0,\n",
    "                 support=None,\n",
    "                 v_max: int = 200,\n",
    "                 v_min: int = 0):\n",
    "        # seed for comparison\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "        # Hyper parameters:\n",
    "        self.state_size = state_size  # Not used only for debugging\n",
    "        self.action_size = action_size  # Not used only for debugging\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.gamma = GAMMA\n",
    "        self.tau = TAU\n",
    "        self.lr = LR\n",
    "        self.UPDATE_MODEL_EVERY = UPDATE_MODEL_EVERY\n",
    "        self.UPDATE_TARGET_EVERY = UPDATE_TARGET_EVERY\n",
    "        self.use_soft_update = use_soft_update\n",
    "        self.priority_method = priority_method\n",
    "        self.t_step = 0\n",
    "\n",
    "        # PER:\n",
    "        self.learn_start = PER_learn_start\n",
    "        self.PER_eps = PER_eps\n",
    "        # Double DQN or QN:\n",
    "        self.model = models[0].to(self.device)\n",
    "        self.model_target = models[1].to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, eps=1.5e-4)\n",
    "\n",
    "        # N-step:\n",
    "        self.n_step = n_step\n",
    "\n",
    "        # Priority Experience Replay:\n",
    "        self.memory = replay_buffer\n",
    "\n",
    "        # Distributional aspect:\n",
    "        # The support for the value distribution. Set to 51 for C51\n",
    "        self.atom_size = atom_size\n",
    "        # Break the range of rewards into 51 uniformly spaced values (support)\n",
    "        self.v_max = v_max\n",
    "        self.v_min = v_min\n",
    "        self.support = support\n",
    "\n",
    "        # plotting:\n",
    "        self.losses = []\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Saves learning experience in memory tree and decides if it is time to update models.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state: The state we are moving from.\n",
    "            action: What action we took.\n",
    "            reward: The reward from going from state to the next state\n",
    "            next_state: The state we end up in.\n",
    "            done: If the game terminated after this step.\n",
    "        \"\"\"\n",
    "        # Save experience in replay memory\n",
    "        if self.priority_method == \"None\":\n",
    "            error = None\n",
    "        elif self.priority_method == \"error\":\n",
    "            error = self.compute_error(state, action, reward, next_state, done)\n",
    "        else:\n",
    "            error = reward\n",
    "        self.memory.add(state, action, reward, next_state, done, error)\n",
    "\n",
    "        # Filling memory\n",
    "        if self.learn_start != 0:\n",
    "            self.learn_start -= 1\n",
    "            if self.learn_start % 1000 == 0:\n",
    "                print(\"\\tFilling memory: \\t{0}\".format(self.learn_start, end=\"\\r\"))\n",
    "            return\n",
    "        # Update t_step:\n",
    "        self.t_step += 1\n",
    "        if self.t_step % self.UPDATE_MODEL_EVERY == 0 and self.t_step % self.UPDATE_TARGET_EVERY == 0:\n",
    "            self.t_step = 0\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        if self.t_step % self.UPDATE_MODEL_EVERY == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                loss = self.learn()\n",
    "                self.losses.append(loss)\n",
    "                if self.use_soft_update:\n",
    "                    self.soft_update(self.model, self.model_target, self.tau)\n",
    "\n",
    "        if not self.use_soft_update and self.t_step % self.UPDATE_TARGET_EVERY == 0:\n",
    "            self.update_target_model()\n",
    "            print(\"\\tTarget model updated\")\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        action_values = self.model.forward(state)\n",
    "        return np.argmax(action_values.detach().cpu().numpy())\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "                Params\n",
    "                ======\n",
    "                    experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "                    gamma (float): discount factor\n",
    "                \"\"\"\n",
    "        # PER sampling:\n",
    "        idxs, experiences, is_weights = self.memory.sample()\n",
    "\n",
    "        # Compute the error or loss:\n",
    "        errors = self._compute_loss(experiences)\n",
    "\n",
    "        # Prepare weights:\n",
    "        is_weights = torch.from_numpy(is_weights).float().to(self.device)\n",
    "\n",
    "        # Compute loss - Per: weighted loss computed and the priorities are updated\n",
    "        loss = torch.mean(is_weights * errors)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update priorities:\n",
    "        self.memory.update_memory_tree(idxs, errors.detach().cpu())\n",
    "\n",
    "        # Resetting the noise of the model\n",
    "        self.model.reset_noise()\n",
    "        self.model_target.reset_noise()\n",
    "        return loss.item()  # Return loss for logging\n",
    "\n",
    "    def _compute_loss(self, experiences):\n",
    "        \"\"\" Computing the loss from the categorical result\n",
    "        \"\"\"\n",
    "        # TODO::Check if gamma should be gamme = gamma ** n_steps\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(\n",
    "            self.device)\n",
    "        dones = torch.from_numpy(\n",
    "            np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(\n",
    "            self.device)\n",
    "\n",
    "        # Categorical DQN algorithm\n",
    "        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)\n",
    "\n",
    "        # TODO: Check if no_grad() here:\n",
    "        with torch.no_grad():\n",
    "            # Double DQN\n",
    "            next_actions = self.model(next_states).argmax(1)\n",
    "            actions = actions.reshape(-1)  # Flatten\n",
    "            next_q_dist = self.model_target.get_distribution(next_states)\n",
    "            next_q_dist = next_q_dist[range(self.batch_size), next_actions]\n",
    "\n",
    "            # Compute the projection of Tˆz onto the support {z}\n",
    "            t_z = rewards + (1 - dones) * self.gamma * self.support\n",
    "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
    "\n",
    "            b = (t_z - self.v_min) / delta_z  # b_j ∈ [0, batch_size − 1]\n",
    "            lower = b.floor().long()\n",
    "            upper = b.ceil().long()\n",
    "\n",
    "            # Distribute probability of Tˆz\n",
    "            offset = (\n",
    "                torch.linspace(\n",
    "                    start=0,\n",
    "                    end=(self.batch_size - 1) * self.atom_size,\n",
    "                    steps=self.batch_size\n",
    "                ).long()\n",
    "                    .unsqueeze(1)\n",
    "                    .expand(self.batch_size, self.atom_size)\n",
    "                    .to(self.device)\n",
    "            )\n",
    "\n",
    "            proj_dist = torch.zeros(next_q_dist.size(), device=self.device)\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                dim=0,\n",
    "                index=(lower + offset).view(-1),\n",
    "                source=(next_q_dist * (upper.float() - b)).view(-1)\n",
    "            )\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                dim=0,\n",
    "                index=(upper + offset).view(-1),\n",
    "                source=(next_q_dist * (b - lower.float())).view(-1)\n",
    "            )\n",
    "\n",
    "        q_dist = self.model.get_distribution(states)\n",
    "        log_p = torch.log(q_dist[range(self.batch_size), actions])\n",
    "        elementwise_loss = -(proj_dist * log_p).sum(1)\n",
    "\n",
    "        return elementwise_loss  # Cross-entropy loss\n",
    "\n",
    "    def compute_error(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Compute the error between model and model_target given one experience\n",
    "        \"\"\"\n",
    "        # Set to eval to avoid backpropergation:\n",
    "        self.model.eval()\n",
    "        self.model_target.eval()\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state).to(self.device)\n",
    "            next_state = torch.from_numpy(next_state).to(self.device)\n",
    "            action = torch.as_tensor(action).to(self.device)\n",
    "            val, max_actions_Snext_local = self.model_target(next_state).detach().max(0)\n",
    "\n",
    "            # Getting the Q-value for these actions (using weight w^-)\n",
    "            Q_targets_next = self.model_target(next_state).detach()[max_actions_Snext_local]\n",
    "\n",
    "            # Compute Q targets for current states (TD-target)\n",
    "            Q_targets = reward + (self.gamma * Q_targets_next * (1 - done))\n",
    "\n",
    "            # Get expected Q values from local model\n",
    "            Q_expected = self.model(state)[action]\n",
    "\n",
    "            error = np.abs((Q_expected - Q_targets).detach().cpu().numpy())\n",
    "        self.model.train()\n",
    "        self.model_target.train()\n",
    "        return error\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\" Hard update model parameters.\n",
    "            Copying the current weights from DDQN to the target model.\n",
    "        \"\"\"\n",
    "        self.model_target.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train and evaluate agent:\n",
    "and helping functions\n",
    "### 6.1 Visualizing and helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingaverage(values, window, mode='same'):\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    sma = np.convolve(values, weights, mode)\n",
    "    return sma.tolist()\n",
    "\n",
    "\n",
    "def plot_score(scores, Ln_blue, Ln_olive, fig):\n",
    "    # print(\"plot:\")\n",
    "    mean_scores = []\n",
    "    mean_scores.append(0)\n",
    "    for i in range(0, len(scores), 5):\n",
    "        l = []\n",
    "        for j in range(5):\n",
    "            l.append(scores[i + j])\n",
    "        mean_scores.append(np.mean(l))\n",
    "    # print(mean_scores)\n",
    "    Ln_blue.set_ydata(mean_scores)\n",
    "    Ln_blue.set_xdata(range(0, len(scores)+1, 5))\n",
    "    if len(scores) >= 30:\n",
    "        yMA = movingaverage(scores, 30)\n",
    "        Ln_olive.set_ydata(yMA)\n",
    "        Ln_olive.set_xdata(range(0, len(scores)))\n",
    "#     display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "    \n",
    "#     plt.pause(0.1)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def unpack_braininfo(brain_name, all_brain_info):\n",
    "    brain_info = all_brain_info[brain_name]\n",
    "    next_state = brain_info.vector_observations[0]\n",
    "    reward = brain_info.rewards[0]\n",
    "    done = brain_info.local_done[0]\n",
    "    max_reached = brain_info.max_reached[0]\n",
    "    return next_state, reward, done, max_reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(agent, brain_name, test_env, n_episodes, train_episode=\"Loaded model\", model_save_file=None,\n",
    "         current_best=10000, set_fast_mode=True):\n",
    "    scores = []  # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    time_window = deque(maxlen=10)  # last 10 iter\n",
    "    agent.model.eval()\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state = test_env.reset(train_mode=set_fast_mode)[brain_name].vector_observations[0]\n",
    "        score = 0\n",
    "        start = time.time()\n",
    "        max_reached = False\n",
    "        while not max_reached:\n",
    "            action = int(agent.act(state))\n",
    "            next_state, reward, done, max_reached = unpack_braininfo(brain_name, test_env.step(action))\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        time_window.append(time.time() - start)\n",
    "        scores_window.append(score)  # save most recent score\n",
    "        scores.append(score)  # save most recent score\n",
    "        print(\n",
    "            '\\rTest: Episode {}\\tAverage Score: {:.2f}\\tthis Score: {:.2f}\\tAverage Time pr episode {:.2f} seconds'.format(\n",
    "                i_episode,\n",
    "                np.mean(\n",
    "                    scores_window),\n",
    "                score,\n",
    "                np.mean(\n",
    "                    time_window)),\n",
    "            end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rTest: Episode {}\\tAverage Score: {:.2f}\\tTime left {:.2f} seconds'.format(i_episode,\n",
    "                                                                                               np.mean(scores_window),\n",
    "                                                                                               np.mean(time_window) * (\n",
    "                                                                                                       n_episodes - i_episode)))\n",
    "            if np.mean(scores_window) >= current_best:\n",
    "                if model_save_file != None:\n",
    "                    torch.save(agent.model.state_dict(), str(model_save_file))\n",
    "                    current_best = np.mean(scores_window)\n",
    "    agent.model.train()\n",
    "    return '\\n\\ttrain_episode: {}\\t Average Score over {} episodes: {}'.format(str(train_episode), str(n_episodes),\n",
    "                                                                               np.mean(scores)), current_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, brain_name, train_env, file, save_img=\"plot.png\", save_file='checkpoint.pth',\n",
    "          n_episodes=2000000, evaluation_interval=200, plot=False, plot_title=\"title\"):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    if plot:\n",
    "        buffer = 1\n",
    "        min_score = 0\n",
    "        max_score = min_score + buffer\n",
    "        fig = plt.figure()\n",
    "        # fig, axs = plt.subplots(2, 1)\n",
    "        # score_ax = axs[0]\n",
    "        score_ax = fig.add_subplot(111)\n",
    "        score_line_blue, = score_ax.plot([0, 0])\n",
    "        score_line_olive, = score_ax.plot([0, 0], color='olive')\n",
    "        score_ax.set_ylim([min_score, max_score])\n",
    "        score_ax.set_xlim([0, 1])\n",
    "\n",
    "        # loss_ax = axs[1]\n",
    "        # loss_line_blue, = loss_ax.plot([0, 0])\n",
    "        # loss_ax.set_ylim([0, 10])\n",
    "        # loss_ax.set_xlim([0, 1])\n",
    "\n",
    "        plt.title(plot_title)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('score mean over 5 epoch')\n",
    "        plt.ion()\n",
    "    scores = []  # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    time_window = deque(maxlen=10)  # last 10 iter\n",
    "    best_avg = 13.0\n",
    "    eval_result = \"\\n## test result: \\n\\n\"\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state = train_env.reset(train_mode=True)[brain_name].vector_observations[0]\n",
    "        score = 0\n",
    "        start = time.time()\n",
    "        max_reached = False\n",
    "        while not max_reached:\n",
    "            action = int(agent.act(state))\n",
    "            next_state, reward, done, max_reached = unpack_braininfo(brain_name, train_env.step(action))\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        time_window.append(time.time() - start)\n",
    "\n",
    "        scores_window.append(score)  # save most recent score\n",
    "        scores.append(score)  # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tthis Score: {:.2f}\\tAverage Time pr episode {:.2f} seconds'.format(\n",
    "            i_episode,\n",
    "            np.mean(\n",
    "                scores_window),\n",
    "            score,\n",
    "            np.mean(\n",
    "                time_window)),\n",
    "            end=\"\")\n",
    "        if plot and i_episode % 5 == 0:\n",
    "            debug = 0\n",
    "            # score\n",
    "            # Update axis (Not working in jupyter notebook, have to delete output each time):\n",
    "#             window = scores[-5:]\n",
    "#             mean = np.mean(window)\n",
    "#             if mean > max_score - buffer:\n",
    "#                 max_score = mean + buffer\n",
    "#                 score_ax.set_ylim([min_score, max_score])\n",
    "#             if mean < min_score + buffer:\n",
    "#                 min_score = mean - buffer\n",
    "#                 score_ax.set_ylim([min_score, max_score])\n",
    "#             score_ax.set_xlim([0, len(scores)])\n",
    "#             # PLOT\n",
    "#             fig = plot_score(scores, score_line_blue, score_line_olive, fig)\n",
    "\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tTime left {:.2f} seconds'.format(i_episode,\n",
    "                                                                                         np.mean(scores_window),\n",
    "                                                                                         np.mean(time_window) * (\n",
    "                                                                                                 n_episodes - i_episode)))\n",
    "            with open(file, \"a+\") as f:\n",
    "                f.write('\\tEpisode {}\\tAverage Score: {:.2f}\\n'.format(i_episode, np.mean(scores_window)))\n",
    "\n",
    "        if i_episode % evaluation_interval == 0:\n",
    "            # Time for evaluation\n",
    "            log_result, current_best = evaluate(agent, brain_name, train_env, 100, i_episode, save_file, best_avg)\n",
    "            eval_result += log_result\n",
    "            best_avg = current_best\n",
    "\n",
    "    with open(file, \"a+\") as f:\n",
    "        f.write(eval_result)\n",
    "        f.write(\"\\n\\nbest score: \" + str(max(scores)) + \" at eps: \" + str(scores.index(max(scores))))\n",
    "    \n",
    "    # Save figure does not work from juputer notebook :(\n",
    "    if plot:\n",
    "        max_score = np.max(scores)\n",
    "        min_score = np.min(scores)\n",
    "        score_ax.set_ylim([min_score, max_score])\n",
    "        score_ax.set_xlim([0, len(scores)])\n",
    "        # PLOT\n",
    "        fig = plot_score(scores, score_line_blue, score_line_olive, fig)\n",
    "        plt.savefig(save_img)\n",
    "            \n",
    "    return scores, best_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time to train:\n",
    "The training result is also saved under the folder **Saved** where a folder is created called **test\\*** and some number. The path will be printed as the first thing when runned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is not executed since use_test_seed=True, Go to 1. Initialise game and change it to run this code\n"
     ]
    }
   ],
   "source": [
    "if not use_test_seed:\n",
    "    # Create plot\n",
    "    plot = True\n",
    "\n",
    "    # Title of plot:\n",
    "    title = \"model: NoisyDueling, agent: rainbow, NSTEP_PER-\" + priority_method + \"Update: \"\n",
    "    if use_soft_update:\n",
    "        title += \"soft\"\n",
    "    else:\n",
    "        title += \"hard\"\n",
    "\n",
    "    # Create data folder:\n",
    "    base_dir = Path(\"saved\", \"test0\")\n",
    "    counter = 0\n",
    "    while base_dir.exists():\n",
    "        counter += 1\n",
    "        base_dir = Path(\"saved\", \"test\" + str(counter))\n",
    "    base_dir.mkdir(parents=True)\n",
    "    file = str(Path(base_dir, \"model_test.md\"))\n",
    "    print(file)\n",
    "    save_file = str(Path(base_dir, \"rainbow_checkpoint.pth\"))\n",
    "    save_image = str(Path(base_dir, \"plot.png\"))\n",
    "\n",
    "    # Write hyperparameters to file\n",
    "    with open(file, \"a+\") as f:\n",
    "        f.write(\"\\n# \" + str(title) + \"\\n\\n\")\n",
    "        f.write(general_info + \"\\n\")\n",
    "        f.write(model_info + \"\\n\")\n",
    "        f.write(agent_info + \"\\n\")\n",
    "        f.write(per_info + \"\\n\")\n",
    "        f.write(train_info + \"\\n\\n\")\n",
    "        f.write(\"\\n## train data: \\n\\n\")\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    support = torch.linspace(v_min, v_max, atom_size).to(device)\n",
    "    # Create models\n",
    "    models = (DistributedNoisyDDQN(state_size, action_size, std_init=std_init, support = support ,\n",
    "                     atom_size=atom_size,seed=seed),\n",
    "              DistributedNoisyDDQN(state_size, action_size, std_init=std_init, support = support ,\n",
    "                     atom_size=atom_size,seed=seed))\n",
    "    # Create N-step PER buffer\n",
    "    replay_buffer = PerNStep(BUFFER_SIZE, \n",
    "                             BATCH_SIZE,\n",
    "                             state_size=state_size,\n",
    "                             seed=seed,\n",
    "                             epsilon=PER_e,\n",
    "                             alpha=PER_a,\n",
    "                             beta=PER_b,\n",
    "                             beta_increase=PER_bi,\n",
    "                             absolute_error_upper=PER_aeu,\n",
    "                             n_step=n_step,\n",
    "                             gamma=GAMMA)\n",
    "\n",
    "    agent = RainbowAgent(state_size,\n",
    "                         action_size,\n",
    "                         models,\n",
    "                         replay_buffer,\n",
    "                         seed=seed,\n",
    "                         BATCH_SIZE=BATCH_SIZE,\n",
    "                         GAMMA=GAMMA,\n",
    "                         TAU=TAU,\n",
    "                         LR=LR, \n",
    "                         UPDATE_MODEL_EVERY=UPDATE_MODEL_EVERY, \n",
    "                         UPDATE_TARGET_EVERY=UPDATE_TARGET_EVERY,\n",
    "                         use_soft_update=use_soft_update,\n",
    "                         priority_method=priority_method,\n",
    "                         PER_learn_start=PER_learn_start,\n",
    "                         PER_eps=PER_e,\n",
    "                         n_step=n_step, \n",
    "                         atom_size = atom_size,\n",
    "                         support = support,\n",
    "                         v_max = v_max,\n",
    "                         v_min = v_min\n",
    "                        )\n",
    "\n",
    "    scores, best_avg = train(agent, brain_name, env, file=file, save_img=save_image, save_file=save_file, n_episodes=episodes, evaluation_interval=evaluation_interval, plot=plot, plot_title=title)\n",
    "else:\n",
    "    print(\" This is not executed since use_test_seed=True, Go to 1. Initialise game and change it to run this code\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Run already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Episode 100\tAverage Score: 13.13\tTime left 0.00 secondsage Time pr episode 2.01 seconds\n"
     ]
    }
   ],
   "source": [
    "if use_test_seed:\n",
    "    model_dir=Path(\"saved\", \"test24\", \"rainbow_checkpoint.pth\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    support = torch.linspace(v_min, v_max, atom_size).to(device)\n",
    "    # Create models\n",
    "    models = [DistributedNoisyDDQN(state_size, action_size, std_init=std_init, support=support,\n",
    "                                   atom_size=atom_size, seed=seed),\n",
    "              DistributedNoisyDDQN(state_size, action_size, std_init=std_init, support=support,\n",
    "                                   atom_size=atom_size, seed=seed)]\n",
    "    # Create N-step PER buffer\n",
    "    replay_buffer = PerNStep(BUFFER_SIZE,\n",
    "                             BATCH_SIZE,\n",
    "                             state_size=state_size,\n",
    "                             seed=seed,\n",
    "                             epsilon=PER_e,\n",
    "                             alpha=PER_a,\n",
    "                             beta=PER_b,\n",
    "                             beta_increase=PER_bi,\n",
    "                             absolute_error_upper=PER_aeu,\n",
    "                             n_step=n_step,\n",
    "                             gamma=GAMMA)\n",
    "\n",
    "    agent = RainbowAgent(state_size,\n",
    "                         action_size,\n",
    "                         models,\n",
    "                         replay_buffer,\n",
    "                         seed=seed,\n",
    "                         BATCH_SIZE=BATCH_SIZE,\n",
    "                         GAMMA=GAMMA,\n",
    "                         TAU=TAU,\n",
    "                         LR=LR,\n",
    "                         UPDATE_MODEL_EVERY=UPDATE_MODEL_EVERY,\n",
    "                         UPDATE_TARGET_EVERY=UPDATE_TARGET_EVERY,\n",
    "                         use_soft_update=use_soft_update,\n",
    "                         priority_method=priority_method,\n",
    "                         PER_learn_start=PER_learn_start,\n",
    "                         PER_eps=PER_e,\n",
    "                         n_step=n_step,\n",
    "                         atom_size=atom_size,\n",
    "                         support=support,\n",
    "                         v_max=v_max,\n",
    "                         v_min=v_min\n",
    "                         )\n",
    "    # Load trained weights into models:\n",
    "    state_dict = torch.load(str(model_dir))\n",
    "    agent.model.load_state_dict(state_dict)\n",
    "    agent.model_target.load_state_dict(state_dict)\n",
    "\n",
    "    # ready for testing:\n",
    "    evaluate(agent, brain_name, env, 100, train_episode=\"Loaded model\", model_save_file=None,\n",
    "             current_best=0, set_fast_mode=False)\n",
    "else:\n",
    "    print(\"This is not executed since use_test_seed=False, Go to 1. Initialise game and change it to run this code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "drl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
