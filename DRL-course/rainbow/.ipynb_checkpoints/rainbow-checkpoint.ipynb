{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation and notes on rainbow DQN\n",
    "This will be some notes on how to build up the Rainbow DQN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualize and data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "# Game env\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# Personal lib:\n",
    "import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. initialise game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityEnvironmentException",
     "evalue": "Couldn't launch the Banana environment. Provided filename does not match any environments.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d6e486cb895d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Game values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mgame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"D:\\dev\\learning\\DRL-project\\DRL-course\\projects\\p1_navigation\\Banana.exe\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_seed\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_test_seed\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_graphics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m# get the default brain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mbrain_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrain_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Fevre\\Lib\\Anaconda3\\envs\\unity\\lib\\site-packages\\unityagents\\environment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file_name, worker_id, base_port, curriculum, seed, docker_training, no_graphics)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# and the communicator will directly try to connect to an existing unity environment.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutable_launcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocker_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_graphics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start training by pressing the Play button in the Unity Editor.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Fevre\\Lib\\Anaconda3\\envs\\unity\\lib\\site-packages\\unityagents\\environment.py\u001b[0m in \u001b[0;36mexecutable_launcher\u001b[1;34m(self, file_name, docker_training, no_graphics)\u001b[0m\n\u001b[0;32m    182\u001b[0m             raise UnityEnvironmentException(\"Couldn't launch the {0} environment. \"\n\u001b[0;32m    183\u001b[0m                                             \u001b[1;34m\"Provided filename does not match any environments.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m                                             .format(true_filename))\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"This is the launch string {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlaunch_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m: Couldn't launch the Banana environment. Provided filename does not match any environments."
     ]
    },
    {
     "ename": "UnityEnvironmentException",
     "evalue": "Couldn't launch the Banana environment. Provided filename does not match any environments.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d6e486cb895d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Game values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mgame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"D:\\dev\\learning\\DRL-project\\DRL-course\\projects\\p1_navigation\\Banana.exe\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_seed\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_test_seed\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_graphics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m# get the default brain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mbrain_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrain_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Fevre\\Lib\\Anaconda3\\envs\\unity\\lib\\site-packages\\unityagents\\environment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file_name, worker_id, base_port, curriculum, seed, docker_training, no_graphics)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# and the communicator will directly try to connect to an existing unity environment.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutable_launcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocker_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_graphics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start training by pressing the Play button in the Unity Editor.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Fevre\\Lib\\Anaconda3\\envs\\unity\\lib\\site-packages\\unityagents\\environment.py\u001b[0m in \u001b[0;36mexecutable_launcher\u001b[1;34m(self, file_name, docker_training, no_graphics)\u001b[0m\n\u001b[0;32m    182\u001b[0m             raise UnityEnvironmentException(\"Couldn't launch the {0} environment. \"\n\u001b[0;32m    183\u001b[0m                                             \u001b[1;34m\"Provided filename does not match any environments.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m                                             .format(true_filename))\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"This is the launch string {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlaunch_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m: Couldn't launch the Banana environment. Provided filename does not match any environments."
     ]
    }
   ],
   "source": [
    "# take test seed:\n",
    "use_test_seed = False\n",
    "test_seed = np.random.randint(low=1, high=1000)\n",
    "\n",
    "#Seed values from now on.\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Game values\n",
    "game = \"C:\\Fevre\\Code\\Python\\DRL-project\\DRL-course\\projects\\p1_navigation\\Banana.exe\"\n",
    "env = UnityEnvironment(file_name=game, seed=test_seed if use_test_seed else seed, no_graphics=False)\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# game info:\n",
    "action_size = brain.vector_action_space_size\n",
    "state = env_info.vector_observations[0]\n",
    "state_size = len(state)\n",
    "\n",
    "general_info = log.create_general_info(\"*general info:*\", game, seed, state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters\n",
    "Here we go through all the hyperparameters\n",
    "### 2.1 model parameters\n",
    "Using a noisy DQN we need to have a initialised noise value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_init = 0.2                      # The amount of noise applied\n",
    "model_info = log.create_model_info(\"*model info:*\", std_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prioritised Experience Replay buffer parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = (2 ** 20)             # The space we use to store Experiences \n",
    "BATCH_SIZE = 512                    # Amount of replays we train on each update.\n",
    "RB_method = \"nstep_per\"             # Choice of replay buffer: nstep_per, (per, replay_buffer)=not_implemented\n",
    "PER_e = 0.01                        # Epsilon\n",
    "PER_a = 0.6                         # Alpha\n",
    "PER_b = 0.4                         # Beta init\n",
    "PER_bi = 0.00001                    # Beta increase is the increase in taking the most prioritiesed replays\n",
    "PER_aeu = 3                         # Absolute error upper is the max priority a replay can have\n",
    "PER_learn_start = 0                 # Used to populated the sumtree with replays\n",
    "n_step = 8                          # Used in the n-step implementation for choosing how many sequent replays we use.\n",
    "per_info = log.create_per_info(\"*per_info:*\", BUFFER_SIZE, BATCH_SIZE, \\\n",
    "                               RB_method, PER_e, PER_a, PER_b, PER_bi, PER_aeu, PER_learn_start, n_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Agent parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99                       # Future discount value\n",
    "TAU = 1e-3                         # Amount we update the target model each update session (use_soft_update=True)\n",
    "LR = 0.00005                       # The learning rate of the model\n",
    "opt_eps = 1.5e-4                   # Adam epsilon (more info)\n",
    "UPDATE_MODEL_EVERY = 10            # The amount of steps between model updates \n",
    "UPDATE_TARGET_EVERY = 8000         # The amount of steps between target updates (use_soft_update=Flase)\n",
    "use_soft_update = True             # Wether we are updating the model using soft updates or copying model weights over.\n",
    "priority_method = \"reward\"         # Initialised priorities (reward, none=max_val, error=compute_error)\n",
    "\n",
    "agent_info = log.create_agent_info(\"*agent info:*\", GAMMA, TAU, LR, opt_eps,\n",
    "                               UPDATE_MODEL_EVERY, UPDATE_TARGET_EVERY, use_soft_update, priority_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 500                    # Number of training episodes\n",
    "evaluation_interval = 200         # Indicating how often we evaluate the current agent.\n",
    "max_t = 1000                      # The max number of steps before going into new episode (not used)\n",
    "train_info = log.create_train_info(\"*train_info:*\", episodes, evaluation_interval, max_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model and layers: Noisy Dueling DQN\n",
    "Noisy DQN is that we apply noise to the last layers to emulated exploration.\n",
    "Dueling DQN is that we split the last layer into two and compute an advangtage vector (How good are each action in this state) and a value (how good is the state). This helps us when training the network to have a... \n",
    "### 3.1 Noisy layer\n",
    "parameter explained:\n",
    "weight_mu:\n",
    "\n",
    "weight_sigma:\n",
    "\n",
    "register_buffer(\"weight_epsilon\"):\n",
    "\n",
    "bias_mu:\n",
    "\n",
    "bias_sigma:\n",
    "\n",
    "register_buffer(\"bias_epsilon\"):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedNoisyLinear(nn.Module):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_features,       # Number of input features\n",
    "                 out_features,      # Number of output features\n",
    "                 std_init,          # Amount of noise in layer\n",
    "                 seed=None,         # The env seed (if needed)     \n",
    "                 name=\"noisyLinear\" # Name for debugging\n",
    "                ):\n",
    "        super(FactorizedNoisyLinear, self).__init__()\n",
    "        if seed is not None:\n",
    "            self.seed = seed\n",
    "            torch.manual_seed(self.seed)\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.register_buffer(\"weight_epsilon\", torch.Tensor(out_features, in_features))\n",
    "        \n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
    "\n",
    "        self.std_init = std_init\n",
    "        self.name = name\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Applying noise to the weights and bias to simulate exploring\n",
    "        :param x: The input state\n",
    "        :return: Linear translation with noisy weights and bias.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "\n",
    "        y = F.linear(x, weight, bias)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-std, std)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "\n",
    "        self.bias_mu.data.uniform_(-std, std)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The noisy DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDDQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, std_init, seed=None):\n",
    "        super(NoisyDDQN, self).__init__()\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.value_stream = nn.Sequential(\n",
    "            FactorizedNoisyLinear(512, 512, std_init=std_init, seed=seed, name=\"value_stream1\"),\n",
    "            nn.ReLU(),\n",
    "            FactorizedNoisyLinear(512, 1, std_init=std_init, seed=seed, name=\"value_stream2\")\n",
    "        )\n",
    "\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            FactorizedNoisyLinear(512, 512, std_init=std_init, seed=seed, name=\"advantage_stream1\"),\n",
    "            nn.ReLU(),\n",
    "            FactorizedNoisyLinear(512, self.action_size, std_init=std_init, seed=seed, name=\"advantage_stream2\")\n",
    "        )\n",
    "\n",
    "        self.feature_layer.apply(self.init_weights)\n",
    "        self.value_stream.apply(self.init_weights)\n",
    "        self.advantage_stream.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.feature_layer(state)\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        q_values = value.expand_as(advantage) + (\n",
    "                advantage - advantage.mean(dim=state.dim() - 1, keepdim=True).expand_as(advantage))\n",
    "        return q_values\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, FactorizedNoisyLinear):\n",
    "                module.reset_noise()\n",
    "\n",
    "    def set_training(self, training):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, FactorizedNoisyLinear):\n",
    "                module.is_training = training\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 N-step Prioritiesed Experience Replay buffer\n",
    "In **the Prioritiesed Experience Replay** (PER) buffer, we do not chose the experience replays at random but weights the less seen or valueable experiences with a higher priority to better cover the information given from this experience.\n",
    "\n",
    "**The N-step** implementation aims to improve the temporal awareness of the agent, by combining the information of current state and action with the next N-steps into the future. This way we try to learn if a state is contributing to future rewards.\n",
    "\n",
    "We store the priorities of the experiences in a sumtree for faster search and updates.\n",
    "### 4.1 Sumtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    def __init__(self, capacity, seed=None):\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        self.capacity = int(capacity)\n",
    "        assert self.is_power_of_2(self.capacity), \"Capacity must be power of 2.\" + str(capacity)\n",
    "        # pointer to current index in data map.\n",
    "        self.data_pointer = 0\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.data_length = 0\n",
    "        # Priority tree.\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_length\n",
    "\n",
    "    def add(self, data, priority):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        # Update the leaf\n",
    "        self.update(tree_index, priority)\n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "        if self.data_length < self.capacity:\n",
    "            self.data_length += 1\n",
    "\n",
    "    def update(self, tree_index, priority):\n",
    "        # change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "\n",
    "    def get_leaf(self, value):\n",
    "        parent_index = 0  # root\n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "\n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else:  # downward search, always search for a higher priority node\n",
    "                if value <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    value -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, data_index, self.tree[leaf_index], self.data[data_index]\n",
    "\n",
    "    # Returns data given a data index\n",
    "    def get_data(self, data_index):\n",
    "        return self.data[data_index % self.capacity]\n",
    "\n",
    "    @staticmethod\n",
    "    def is_power_of_2(n):\n",
    "        return ((n & (n - 1)) == 0) and n != 0\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0]  # the root\n",
    "\n",
    "    @property\n",
    "    def max_priority(self):\n",
    "        return np.max(self.tree[-self.data_length:])\n",
    "\n",
    "    @property\n",
    "    def min_priority(self):\n",
    "        return np.min(self.tree[-self.data_length:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 N-step PER buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerNStep:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __init__(self, capacity, batch_size, state_size, seed=None, epsilon=.001, alpha=.6, beta=.4, beta_increase=1e-3,\n",
    "                 absolute_error_upper=3, n_step=3, gamma=.99):\n",
    "        \"\"\"\n",
    "        :param capacity: Max amount of experience saved in the structure\n",
    "        :param epsilon: small value to insure all probabilities is not 0\n",
    "        :param alpha: introduces some randomness and to insure we don't train the same experience and overfit\n",
    "                      alpha=1 means greedy selecting the experience with highest priority\n",
    "                      alpha=0 means pure uniform randomness\n",
    "        :param beta: controls how much IS w affect learning\n",
    "                     beta>=0, starts close to 0 and get closer and closer to 1\n",
    "                     because these weights are more important in the end of learning when our q-values\n",
    "                     begins to convert\n",
    "        :param beta_increase: is the increase in beta for each sampling. 0.001 = 1e-3\n",
    "        :param absolute_error_upper: Setting a cap on how big an error (priority) can be.\n",
    "        :param n_step: store the most recent n-step transitions or experiences instead of the default 1.\n",
    "        :param gamma: This is the discount value\n",
    "        \"\"\"\n",
    "        ## Just like PER\n",
    "        # seeding\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "        # init\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.memory_tree = SumTree(self.capacity, seed)\n",
    "        self.experience = namedtuple(\"Experience\",\n",
    "                                     field_names=[\"timestep\", \"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_increase = beta_increase\n",
    "        self.absolute_error_upper = absolute_error_upper\n",
    "        self.seed = seed\n",
    "\n",
    "        ## N-Step\n",
    "        self.t = 0  # Internal time step counter\n",
    "        self.n_step = n_step\n",
    "        self.n_step_buff = deque(maxlen=n_step)\n",
    "        self.gamma = gamma\n",
    "        self.blank_experience = self.experience(timestep=0,\n",
    "                                                state=torch.zeros(state_size, dtype=torch.float64),\n",
    "                                                action=None,\n",
    "                                                reward=0,\n",
    "                                                next_state=torch.zeros(state_size, dtype=torch.float64),\n",
    "                                                done=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory_tree)\n",
    "\n",
    "    def is_full(self):\n",
    "        return len(self.memory_tree) >= self.memory_tree.capacity\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "                - First, to sample a minibatch of size k the range [0, priority_total] is divided into k ranges.\n",
    "                - Then a value is uniformly sampled from each range.\n",
    "                - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "                - Then, we calculate IS weights for each minibatch element.\n",
    "\n",
    "                The difference here from the last structure is that we need to move to device\n",
    "                in the method calling this function.\n",
    "\n",
    "                so an example:\n",
    "                idxs, experiences, is_weights = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "                states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(device)\n",
    "                actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).float().to(device)\n",
    "                rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(device)\n",
    "                next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(device)\n",
    "                dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "                is_weights =  torch.from_numpy(is_weights).float().to(device)\n",
    "                \"\"\"\n",
    "\n",
    "        minibatch = []\n",
    "\n",
    "        idxs = np.empty((self.batch_size,), dtype=np.int32)\n",
    "        is_weights = np.empty((self.batch_size,), dtype=np.float32)\n",
    "\n",
    "        # Calculate the priority segment\n",
    "        # Divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.memory_tree.total_priority / self.batch_size  # priority segment\n",
    "\n",
    "        # Increase the beta each time we sample a new minibatch\n",
    "        self.beta = np.amin([1., self.beta + self.beta_increase])  # max = 1\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            \"\"\"\n",
    "            A value is uniformly sampled from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "\n",
    "            # This while is to counter that we find a leaf that is not populated yet.\n",
    "            # It happens when the buffer size is very large.\n",
    "            leaf_index, data_index, priority, data = 0, 0, 0, 0\n",
    "            while data == 0:\n",
    "                value = np.random.uniform(a, b)\n",
    "\n",
    "                \"\"\"\n",
    "                Experience that corresponds to each value is retrieved\n",
    "                \"\"\"\n",
    "                leaf_index, data_index, priority, data = self.memory_tree.get_leaf(value)\n",
    "\n",
    "            # P(i) = p_i**a / sum_k p_k**a\n",
    "            sampling_probabilities = priority / self.memory_tree.total_priority\n",
    "\n",
    "            # (1/N * 1/P(i))**b and to normalize it we divide with max_weight\n",
    "            # So is_weights[i] = (1/N * 1/P(i))**b\n",
    "            is_weights[i] = np.power(self.batch_size * sampling_probabilities, -self.beta)\n",
    "\n",
    "            idxs[i] = leaf_index\n",
    "            minibatch.append(data)\n",
    "        is_weights /= is_weights.max()\n",
    "        return idxs, minibatch, is_weights\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done, error=None):\n",
    "        exp = self.experience(self.t, torch.from_numpy(state), action, reward, torch.from_numpy(next_state), done)\n",
    "        self.n_step_buff.append(exp)\n",
    "        self.t = (0 if done else self.t + 1)\n",
    "        if len(self.n_step_buff) < self.n_step:\n",
    "            return None\n",
    "        exp, priority = self._get_n_step_info(self.n_step_buff, self.gamma)\n",
    "        priority = min((abs(priority) + self.epsilon) ** self.alpha, self.absolute_error_upper)\n",
    "        self.memory_tree.add(exp, priority)\n",
    "\n",
    "    def update_memory_tree(self, idxs, errors):\n",
    "        errors = errors + self.epsilon\n",
    "        clipped_errors = np.minimum(errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.alpha)\n",
    "\n",
    "        for idx, p in zip(idxs, ps):\n",
    "            self.memory_tree.update(idx, p)\n",
    "\n",
    "    def _get_n_step_info(self, n_step_buff, gamma):\n",
    "        timestep, org_state, org_action, _, _, _ = n_step_buff[0]\n",
    "        relevant_transitions = []\n",
    "        for transition in list(n_step_buff):\n",
    "            if timestep == transition.timestep:\n",
    "                relevant_transitions.append(transition)\n",
    "                timestep += 1\n",
    "            else:\n",
    "                break\n",
    "        # Take last element in deque and add the reward\n",
    "        rew, n_state, done = relevant_transitions[-1][-3:]\n",
    "        for transition in reversed(relevant_transitions[:-1]):\n",
    "            reward, n_s, d = transition.reward, transition.next_state, transition.done\n",
    "            rew = reward + gamma * rew * (1 - done)\n",
    "            n_state, done = (n_s, d) if d else (n_state, done)\n",
    "        return self.experience(timestep, org_state, org_action, rew, n_state, done), rew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distributional Double DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowAgent:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_size, action_size, models, replay_buffer, seed=None, BATCH_SIZE=64,\n",
    "                 GAMMA=0.99, TAU=1e-3, LR=5e-4, UPDATE_MODEL_EVERY=4, UPDATE_TARGET_EVERY=1000,\n",
    "                 use_soft_update=False, priority_method=\"reward\", PER_learn_start=0):\n",
    "        # seed for comparison\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "            torch.manual_seed(self.seed)\n",
    "        # Hyper parameters:\n",
    "        self.state_size = state_size                       # Not used only for debugging\n",
    "        self.action_size = action_size                     # Not used only for debugging\n",
    "        self.batch_size = BATCH_SIZE                       \n",
    "        self.gamma = GAMMA\n",
    "        self.tau = TAU\n",
    "        self.lr = LR\n",
    "        self.UPDATE_MODEL_EVERY = UPDATE_MODEL_EVERY\n",
    "        self.UPDATE_TARGET_EVERY = UPDATE_TARGET_EVERY\n",
    "        self.use_soft_update = use_soft_update\n",
    "        self.priority_method = priority_method\n",
    "        self.t_step = 0\n",
    "        self.learn_start = PER_learn_start\n",
    "\n",
    "        # Double DQN or QN:\n",
    "        self.model = models[0].to(self.device)\n",
    "        self.model_target = models[1].to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, eps=1.5e-4)\n",
    "\n",
    "        # N-step:\n",
    "        self.n_step = n_step\n",
    "\n",
    "        # Priority Experience Replay:\n",
    "        self.memory = replay_buffer\n",
    "        \n",
    "        # plotting:\n",
    "        self.losses = []\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Saves learning experience in memory tree and decides if it is time to update models.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state: The state we are moving from.\n",
    "            action: What action we took.\n",
    "            reward: The reward from going from state to the next state\n",
    "            next_state: The state we end up in.\n",
    "            done: If the game terminated after this step.\n",
    "        \"\"\"\n",
    "        # Save experience in replay memory\n",
    "        if self.priority_method == \"None\":\n",
    "            error = None\n",
    "        elif self.priority_method == \"error\":\n",
    "            error = self.compute_error(state, action, reward, next_state, done)\n",
    "        else:\n",
    "            error = reward\n",
    "        self.memory.add(state, action, reward, next_state, done, error)\n",
    "\n",
    "        # Filling memory\n",
    "        if self.learn_start != 0:\n",
    "            self.learn_start -= 1\n",
    "            if self.learn_start % 1000 == 0:\n",
    "                print(\"\\tFilling memory: \\t{0}\".format(self.learn_start, end=\"\\r\"))\n",
    "            return\n",
    "        # Update t_step:\n",
    "        self.t_step += 1\n",
    "        if self.t_step % self.UPDATE_MODEL_EVERY == 0 and self.t_step % self.UPDATE_TARGET_EVERY == 0:\n",
    "            self.t_step = 0\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        if self.t_step % self.UPDATE_MODEL_EVERY == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                loss = self.learn()\n",
    "                self.losses.append(loss)\n",
    "                if self.use_soft_update:\n",
    "                    self.soft_update(self.model, self.model_target, self.tau)\n",
    "\n",
    "        if not self.use_soft_update and self.t_step % self.UPDATE_TARGET_EVERY == 0:\n",
    "            self.update_target_model()\n",
    "            print(\"\\tTarget model updated\")\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        # TODO: Question for reviewers, should i disable noise here? hence put it on eval mode\n",
    "        #       I do see a faster growth in avg score in shorter training period,\n",
    "        #       but this might be from less exploring, hence might be bad in the longer run.\n",
    "        #       The noise is applied to the sampled memories under model update,\n",
    "        #       so this is why we might not need it here?\n",
    "        # self.model.eval() # disable noise\n",
    "        with torch.no_grad():\n",
    "            action_values = self.model.forward(state)\n",
    "        # self.model.train() # enable noise\n",
    "        return np.argmax(action_values.detach().cpu().numpy())\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "                Params\n",
    "                ======\n",
    "                    experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "                    gamma (float): discount factor\n",
    "                \"\"\"\n",
    "        # PER:\n",
    "        idxs, experiences, is_weights = self.memory.sample()\n",
    "        is_weights = torch.from_numpy(is_weights).float().to(self.device)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(\n",
    "            self.device)\n",
    "        dones = torch.from_numpy(\n",
    "            np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(\n",
    "            self.device)\n",
    "        \n",
    "        #todo:: Test if this is ok.\n",
    "        # with torch.no_grad():\n",
    "        \n",
    "        # Getting the max action of local network (using weights w)\n",
    "        max_actions = self.model.forward(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "\n",
    "        # Getting the Q-value for these actions (using weight w^-)\n",
    "        Q_targets_next = self.model_target.forward(next_states).detach().gather(1, max_actions)\n",
    "\n",
    "        # Compute Q targets for current states (TD-target)\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.model(states).gather(1, actions)\n",
    "\n",
    "        \n",
    "        # Compute loss - Per: weighted loss computed and the priorities are updated\n",
    "        errors = torch.abs(Q_expected - Q_targets).detach().cpu()\n",
    "        self.memory.update_memory_tree(idxs, errors)\n",
    "        loss = (is_weights * F.mse_loss(Q_expected, Q_targets)).mean()\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # Resetting the noise of the model\n",
    "        self.model.reset_noise()\n",
    "        self.model_target.reset_noise()\n",
    "        return loss.item() # Return loss for logging\n",
    "\n",
    "    def compute_error(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Compute the error between model and model_target given one experience\n",
    "        \"\"\"\n",
    "        # Set to eval to avoid backpropergation:\n",
    "        self.model.eval() \n",
    "        self.model_target.eval()\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state).to(self.device)\n",
    "            next_state = torch.from_numpy(next_state).to(self.device)\n",
    "            action = torch.as_tensor(action).to(self.device)\n",
    "            val, max_actions_Snext_local = self.model_target(next_state).detach().max(0)\n",
    "\n",
    "            # Getting the Q-value for these actions (using weight w^-)\n",
    "            Q_targets_next = self.model_target(next_state).detach()[max_actions_Snext_local]\n",
    "\n",
    "            # Compute Q targets for current states (TD-target)\n",
    "            Q_targets = reward + (self.gamma * Q_targets_next * (1 - done))\n",
    "\n",
    "            # Get expected Q values from local model\n",
    "            Q_expected = self.model(state)[action]\n",
    "\n",
    "            error = np.abs((Q_expected - Q_targets).detach().cpu().numpy())\n",
    "        self.model.train()\n",
    "        self.model_target.train()\n",
    "        return error\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\" Hard update model parameters.\n",
    "            Copying the current weights from DDQN to the target model.\n",
    "        \"\"\"\n",
    "        self.model_target.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train and evaluate agent:\n",
    "and helping functions\n",
    "### 6.1 Visualizing and helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingaverage(values, window, mode='same'):\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    sma = np.convolve(values, weights, mode)\n",
    "    return sma.tolist()\n",
    "\n",
    "\n",
    "def plot_score(scores, Ln_blue, Ln_olive, fig):\n",
    "    # print(\"plot:\")\n",
    "    mean_scores = []\n",
    "    mean_scores.append(0)\n",
    "    for i in range(0, len(scores), 5):\n",
    "        l = []\n",
    "        for j in range(5):\n",
    "            l.append(scores[i + j])\n",
    "        mean_scores.append(np.mean(l))\n",
    "    # print(mean_scores)\n",
    "    Ln_blue.set_ydata(mean_scores)\n",
    "    Ln_blue.set_xdata(range(0, len(scores)+1, 5))\n",
    "    if len(scores) >= 30:\n",
    "        yMA = movingaverage(scores, 30)\n",
    "        Ln_olive.set_ydata(yMA)\n",
    "        Ln_olive.set_xdata(range(0, len(scores)))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "    \n",
    "    plt.pause(0.1)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def unpack_braininfo(brain_name, all_brain_info):\n",
    "    brain_info = all_brain_info[brain_name]\n",
    "    next_state = brain_info.vector_observations[0]\n",
    "    reward = brain_info.rewards[0]\n",
    "    done = brain_info.local_done[0]\n",
    "    max_reached = brain_info.max_reached[0]\n",
    "    return next_state, reward, done, max_reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(agent, brain_name, test_env, n_episodes, train_episode=\"Loaded model\", model_save_file=None,\n",
    "         current_best=10000, set_fast_mode=True):\n",
    "    scores = []  # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    time_window = deque(maxlen=10)  # last 10 iter\n",
    "    agent.model.eval()\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state = test_env.reset(train_mode=set_fast_mode)[brain_name].vector_observations[0]\n",
    "        score = 0\n",
    "        start = time.time()\n",
    "        max_reached = False\n",
    "        while not max_reached:\n",
    "            action = int(agent.act(state))\n",
    "            next_state, reward, done, max_reached = unpack_braininfo(brain_name, test_env.step(action))\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        time_window.append(time.time() - start)\n",
    "        scores_window.append(score)  # save most recent score\n",
    "        scores.append(score)  # save most recent score\n",
    "        print(\n",
    "            '\\rTest: Episode {}\\tAverage Score: {:.2f}\\tthis Score: {:.2f}\\tAverage Time pr episode {:.2f} seconds'.format(\n",
    "                i_episode,\n",
    "                np.mean(\n",
    "                    scores_window),\n",
    "                score,\n",
    "                np.mean(\n",
    "                    time_window)),\n",
    "            end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('Test: \\rEpisode {}\\tAverage Score: {:.2f}\\tTime left {:.2f} seconds'.format(i_episode,\n",
    "                                                                                               np.mean(scores_window),\n",
    "                                                                                               np.mean(time_window) * (\n",
    "                                                                                                       n_episodes - i_episode)))\n",
    "            if np.mean(scores_window) >= current_best:\n",
    "                if model_save_file != None:\n",
    "                    torch.save(agent.model.state_dict(), str(model_save_file))\n",
    "                    current_best = np.mean(scores_window)\n",
    "    agent.model.train()\n",
    "    return '\\n\\ttrain_episode: {}\\t Average Score over {} episodes: {}'.format(str(train_episode), str(n_episodes),\n",
    "                                                                               np.mean(scores)), current_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, brain_name, train_env, file, save_img=\"plot.png\", save_file='checkpoint.pth',\n",
    "          n_episodes=2000000, evaluation_interval=200, plot=False, plot_title=\"title\"):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    if plot:\n",
    "        buffer = 1\n",
    "        min_score = 0\n",
    "        max_score = min_score + buffer\n",
    "        fig = plt.figure()\n",
    "        # fig, axs = plt.subplots(2, 1)\n",
    "        # score_ax = axs[0]\n",
    "        score_ax = fig.add_subplot(111)\n",
    "        score_line_blue, = score_ax.plot([0, 0])\n",
    "        score_line_olive, = score_ax.plot([0, 0], color='olive')\n",
    "        score_ax.set_ylim([min_score, max_score])\n",
    "        score_ax.set_xlim([0, 1])\n",
    "\n",
    "        # loss_ax = axs[1]\n",
    "        # loss_line_blue, = loss_ax.plot([0, 0])\n",
    "        # loss_ax.set_ylim([0, 10])\n",
    "        # loss_ax.set_xlim([0, 1])\n",
    "\n",
    "        plt.title(plot_title)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('score mean over 5 epoch')\n",
    "        plt.ion()\n",
    "        plt.show()\n",
    "    scores = []  # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    time_window = deque(maxlen=10)  # last 10 iter\n",
    "    best_avg = 13.0\n",
    "    eval_result = \"\\n## test result: \\n\\n\"\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        state = train_env.reset(train_mode=True)[brain_name].vector_observations[0]\n",
    "        score = 0\n",
    "        start = time.time()\n",
    "        max_reached = False\n",
    "        while not max_reached:\n",
    "            action = int(agent.act(state))\n",
    "            next_state, reward, done, max_reached = unpack_braininfo(brain_name, train_env.step(action))\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        time_window.append(time.time() - start)\n",
    "\n",
    "        scores_window.append(score)  # save most recent score\n",
    "        scores.append(score)  # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tthis Score: {:.2f}\\tAverage Time pr episode {:.2f} seconds'.format(\n",
    "            i_episode,\n",
    "            np.mean(\n",
    "                scores_window),\n",
    "            score,\n",
    "            np.mean(\n",
    "                time_window)),\n",
    "            end=\"\")\n",
    "        if plot and i_episode % 5 == 0:\n",
    "            # score\n",
    "            # Update axis:\n",
    "            window = scores[-5:]\n",
    "            mean = np.mean(window)\n",
    "            if mean > max_score - buffer:\n",
    "                max_score = mean + buffer\n",
    "                score_ax.set_ylim([min_score, max_score])\n",
    "            if mean < min_score + buffer:\n",
    "                min_score = mean - buffer\n",
    "                score_ax.set_ylim([min_score, max_score])\n",
    "            score_ax.set_xlim([0, len(scores)])\n",
    "            # PLOT\n",
    "            fig = plot_score(scores, score_line_blue, score_line_olive, fig)\n",
    "\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tTime left {:.2f} seconds'.format(i_episode,\n",
    "                                                                                         np.mean(scores_window),\n",
    "                                                                                         np.mean(time_window) * (\n",
    "                                                                                                 n_episodes - i_episode)))\n",
    "            with open(file, \"a+\") as f:\n",
    "                f.write('\\tEpisode {}\\tAverage Score: {:.2f}\\n'.format(i_episode, np.mean(scores_window)))\n",
    "            if plot:\n",
    "                plt.savefig(save_img)\n",
    "\n",
    "        if np.mean(scores_window) >= best_avg:\n",
    "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}\\tTime left {:.2f} seconds'.format(\n",
    "#                 i_episode,\n",
    "#                 np.mean(scores_window), np.mean(time_window) * (n_episodes - i_episode)))\n",
    "            # log_result, current_best = eval(agent, brain_name, train_env, 100, i_episode, save_file, best_avg)\n",
    "            # eval_result += log_result\n",
    "            # best_avg = current_best\n",
    "            debug =0\n",
    "\n",
    "        if i_episode % evaluation_interval == 0:\n",
    "            # Time for evaluation\n",
    "            log_result, current_best = evaluate(agent, brain_name, train_env, 100, i_episode, save_file, best_avg)\n",
    "            eval_result += log_result\n",
    "            best_avg = current_best\n",
    "\n",
    "    with open(file, \"a+\") as f:\n",
    "        f.write(eval_result)\n",
    "        f.write(\"\\n\\nbest score: \" + str(max(scores)) + \" at eps: \" + str(scores.index(max(scores))))\n",
    "    return scores, best_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "plot = True\n",
    "\n",
    "# Title of plot:\n",
    "title = \"model: NoisyDueling, agent: rainbow, NSTEP_PER-\" + priority_method + \"Update: \"\n",
    "if use_soft_update:\n",
    "    title += \"soft\"\n",
    "else:\n",
    "    title += \"hard\"\n",
    "\n",
    "# Create data folder:\n",
    "base_dir = Path(\"saved\", \"test0\")\n",
    "counter = 0\n",
    "while base_dir.exists():\n",
    "    counter += 1\n",
    "    base_dir = Path(\"saved\", \"test\" + str(counter))\n",
    "base_dir.mkdir(parents=True)\n",
    "file = str(Path(base_dir, \"model_test.md\"))\n",
    "print(file)\n",
    "save_file = str(Path(base_dir, \"rainbow_checkpoint.pth\"))\n",
    "save_image = str(Path(base_dir, \"plot.png\"))\n",
    "\n",
    "# Write hyperparameters to file\n",
    "with open(file, \"a+\") as f:\n",
    "    f.write(\"\\n# \" + str(title) + \"\\n\\n\")\n",
    "    f.write(general_info + \"\\n\")\n",
    "    f.write(model_info + \"\\n\")\n",
    "    f.write(agent_info + \"\\n\")\n",
    "    f.write(per_info + \"\\n\")\n",
    "    f.write(train_info + \"\\n\\n\")\n",
    "    f.write(\"\\n## train data: \\n\\n\")\n",
    "    \n",
    "# Create models\n",
    "models = (NoisyDDQN(state_size, action_size, std_init=std_init, seed=seed),\n",
    "          NoisyDDQN(state_size, action_size, std_init=std_init, seed=seed))\n",
    "# Create N-step PER buffer\n",
    "replay_buffer = PerNStep(BUFFER_SIZE, \n",
    "                         BATCH_SIZE,\n",
    "                         state_size=state_size,\n",
    "                         seed=seed,\n",
    "                         epsilon=PER_e,\n",
    "                         alpha=PER_a,\n",
    "                         beta=PER_b,\n",
    "                         beta_increase=PER_bi,\n",
    "                         absolute_error_upper=PER_aeu,\n",
    "                         n_step=n_step,\n",
    "                         gamma=GAMMA)\n",
    "\n",
    "agent = RainbowAgent(state_size,\n",
    "                     action_size,\n",
    "                     models,\n",
    "                     replay_buffer,\n",
    "                     seed=seed,\n",
    "                     BATCH_SIZE=BATCH_SIZE,\n",
    "                     GAMMA=GAMMA,\n",
    "                     TAU=TAU,\n",
    "                     LR=LR, \n",
    "                     UPDATE_MODEL_EVERY=UPDATE_MODEL_EVERY, \n",
    "                     UPDATE_TARGET_EVERY=UPDATE_TARGET_EVERY,\n",
    "                     use_soft_update=use_soft_update,\n",
    "                     priority_method=priority_method,\n",
    "                     PER_learn_start=PER_learn_start)\n",
    "\n",
    "train(agent, brain_name, env, file=file, save_img=save_image, save_file=save_file, n_episodes=episodes,\n",
    "      evaluation_interval=evaluation_interval, plot=plot, plot_title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity",
   "language": "python",
   "name": "unity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
